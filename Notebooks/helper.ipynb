{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c098518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01357fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_node_label(filename, skip_head=False):\n",
    "    fin = open(filename, 'r')\n",
    "    X = []\n",
    "    Y = []\n",
    "    while 1:\n",
    "        if skip_head:\n",
    "            fin.readline()\n",
    "        l = fin.readline()\n",
    "        if l == '':\n",
    "            break\n",
    "        vec = l.strip().split(' ')\n",
    "        X.append(vec[0])\n",
    "        Y.append(vec[1:])\n",
    "    fin.close()\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b5f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return np.asarray(all_labels)\n",
    "    \n",
    "def split_train_evaluate(X, Y, embeddings, train_precent, seed=0):\n",
    "    state = np.random.get_state()\n",
    "    training_size = int(train_precent * len(X))\n",
    "    np.random.seed(seed)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(X)))\n",
    "    X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
    "    Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
    "    X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "    Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "\n",
    "    # train\n",
    "    binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "    binarizer.fit(Y)\n",
    "    X_tr = [embeddings[x] for x in X_train]\n",
    "    Y_tr = binarizer.transform(Y_train)\n",
    "    clf=TopKRanker(LogisticRegression())\n",
    "    clf.fit(X_tr, Y_tr)\n",
    "\n",
    "    np.random.set_state(state)\n",
    "    top_k_list = [len(l) for l in Y_test]\n",
    "    X_test = np.asarray([embeddings[x] for x in X_test])\n",
    "    Y_pred = clf.predict(X_test, top_k_list=top_k_list)\n",
    "    results = {}\n",
    "    results['acc'] = accuracy_score(binarizer.transform(Y_test),Y_pred)\n",
    "    print('-------------------')\n",
    "    print(results)\n",
    "    print('-------------------')\n",
    "    \n",
    "    return results['acc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf16a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# healpers for node2vec\n",
    "def create_alias_table(area_ratio):\n",
    "    \"\"\"\n",
    "    :param area_ratio: sum(area_ratio)=1\n",
    "    :return: accept,alias\n",
    "    \"\"\"\n",
    "    l = len(area_ratio)\n",
    "    accept, alias = [0] * l, [0] * l\n",
    "    small, large = [], []\n",
    "    area_ratio_ = np.array(area_ratio) * l\n",
    "    for i, prob in enumerate(area_ratio_):\n",
    "        if prob < 1.0:\n",
    "            small.append(i)\n",
    "        else:\n",
    "            large.append(i)\n",
    "\n",
    "    while small and large:\n",
    "        small_idx, large_idx = small.pop(), large.pop()\n",
    "        accept[small_idx] = area_ratio_[small_idx]\n",
    "        alias[small_idx] = large_idx\n",
    "        area_ratio_[large_idx] = area_ratio_[large_idx] - \\\n",
    "            (1 - area_ratio_[small_idx])\n",
    "        if area_ratio_[large_idx] < 1.0:\n",
    "            small.append(large_idx)\n",
    "        else:\n",
    "            large.append(large_idx)\n",
    "\n",
    "    while large:\n",
    "        large_idx = large.pop()\n",
    "        accept[large_idx] = 1\n",
    "    while small:\n",
    "        small_idx = small.pop()\n",
    "        accept[small_idx] = 1\n",
    "\n",
    "    return accept, alias\n",
    "\n",
    "def alias_sample(accept, alias):\n",
    "    \"\"\"\n",
    "\n",
    "    :param accept:\n",
    "    :param alias:\n",
    "    :return: sample index\n",
    "    \"\"\"\n",
    "    N = len(accept)\n",
    "    i = int(np.random.random()*N)\n",
    "    r = np.random.random()\n",
    "    if r < accept[i]:\n",
    "        return i\n",
    "    else:\n",
    "        return alias[i]\n",
    "    \n",
    "def get_alias_edge(G, p, q, t, v):\n",
    "        unnormalized_probs = []\n",
    "        for x in G.neighbors(v):\n",
    "            weight = G[v][x].get('weight', 1.0)  # w_vx\n",
    "            if x == t:  # d_tx == 0\n",
    "                unnormalized_probs.append(weight/p)\n",
    "            elif G.has_edge(x, t):  # d_tx == 1\n",
    "                unnormalized_probs.append(weight)\n",
    "            else:  # d_tx > 1\n",
    "                unnormalized_probs.append(weight/q)\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs = [\n",
    "            float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "        return create_alias_table(normalized_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b466fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(G, walks, embed_size=128, window_size=5, workers=3, iter=5, **kwargs):\n",
    "    kwargs[\"sentences\"] = walks\n",
    "    kwargs[\"min_count\"] = kwargs.get(\"min_count\", 0)\n",
    "    kwargs[\"vector_size\"] = embed_size\n",
    "    kwargs[\"sg\"] = 1  # skip gram\n",
    "    kwargs[\"hs\"] = 1  # deepwalk use Hierarchical Softmax\n",
    "    kwargs[\"workers\"] = workers\n",
    "    kwargs[\"window\"] = window_size\n",
    "    kwargs[\"epochs\"] = iter\n",
    "\n",
    "    print(\"Learning embedding vectors...\")\n",
    "    model = Word2Vec(**kwargs)\n",
    "    print(\"Learning embedding vectors done!\")\n",
    "\n",
    "    embeddings = {}\n",
    "    for word in G.nodes():\n",
    "        embeddings[word] = model.wv[word]\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e343b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepwalk_walks(G, num_walks, walk_length,):\n",
    "        nodes = G.nodes()\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            for v in nodes:\n",
    "                walk = [v]\n",
    "                while len(walk) < walk_length:\n",
    "                    cur = walk[-1]\n",
    "                    cur_nbrs = list(G.neighbors(cur))\n",
    "                    if len(cur_nbrs) > 0:\n",
    "                        walk.append(random.choice(cur_nbrs))\n",
    "                    else:\n",
    "                        break\n",
    "                walks.append(walk)\n",
    "        return walks\n",
    "    \n",
    "def node2vec_walks(G, p, q, num_walks, walk_length,):\n",
    "        alias_nodes = {}\n",
    "        for node in G.nodes():\n",
    "            unnormalized_probs = [G[node][nbr].get('weight', 1.0)\n",
    "                                  for nbr in G.neighbors(node)]\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            normalized_probs = [\n",
    "                float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "            alias_nodes[node] = create_alias_table(normalized_probs)\n",
    "\n",
    "        alias_edges = {}\n",
    "\n",
    "        for edge in G.edges():\n",
    "            alias_edges[edge] = get_alias_edge(G, p, q, edge[0], edge[1])\n",
    "        \n",
    "        # generate walks\n",
    "        nodes = G.nodes()\n",
    "        walks = []\n",
    "        for _ in range(num_walks):\n",
    "            for v in nodes:\n",
    "                walk = [v]\n",
    "                while len(walk) < walk_length:\n",
    "                    cur = walk[-1]\n",
    "                    cur_nbrs = list(G.neighbors(cur))\n",
    "                    if len(cur_nbrs) > 0:\n",
    "                        if len(walk) == 1:\n",
    "                            walk.append(\n",
    "                                cur_nbrs[alias_sample(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "                        else:\n",
    "                            prev = walk[-2]\n",
    "                            edge = (prev, cur)\n",
    "                            next_node = cur_nbrs[alias_sample(alias_edges[edge][0],\n",
    "                                                      alias_edges[edge][1])]\n",
    "                            walk.append(next_node)\n",
    "                    else:\n",
    "                        break\n",
    "                walks.append(walk)\n",
    "        return walks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe4fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(embeddings, label):\n",
    "\n",
    "    X, Y = read_node_label(label,skip_head=True)\n",
    "    emb_list = []\n",
    "    for k in X:\n",
    "        emb_list.append(embeddings[k])\n",
    "    emb_list = np.array(emb_list)\n",
    "\n",
    "    model = TSNE(n_components=2)\n",
    "    node_pos = model.fit_transform(emb_list)\n",
    "    color_idx = {}\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        color_idx.setdefault(Y[i][0], [])\n",
    "        color_idx[Y[i][0]].append(i)\n",
    "\n",
    "    for c, idx in color_idx.items():\n",
    "        plt.scatter(node_pos[idx, 0], node_pos[idx, 1], label=c)  # c=node_colors)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2cc65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b99e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "697eee3d7a9e7bae146dcaf591019fe1fe81ba09a3f6d8f0bc2398f5d22d23bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
